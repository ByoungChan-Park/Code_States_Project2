{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning or RuntimeWarning)\n",
    "DATA_PATH = \"../model_code/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CB 모델의 성능을 검증한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리상에 여유를 위해 pickling 하는 함수\n",
    "def pickling(arg_object, arg_file_name):\n",
    "    with open(f'{arg_file_name}.pkl','wb') as pickle_file:\n",
    "        pickle.dump(arg_object, pickle_file)       \n",
    "    print(f\"{arg_file_name}.pkl로 pickling 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 pkl 파일을 불러오는 함수\n",
    "def test_pkl(name):\n",
    "    test = None\n",
    "    with open(f'{name}.pkl','rb') as pickle_file:\n",
    "        test = pickle.load(pickle_file)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 카테고리별로 pearson 유사도를 계산해서 해당 유저가 가장 많이 본 상품과 비슷한 상품 10개를 추천한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appliances_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furniture_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computers_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electronics_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apparel_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construction_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kids_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accessories_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medicine_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stationery_pearson_table.pkl로 pickling 완료\n",
      "country_yard_pearson_table.pkl로 pickling 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\havea\\AppData\\Local\\Temp/ipykernel_14980/1209434548.py:79: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n"
     ]
    }
   ],
   "source": [
    "# 각 카테고리별로 pearson table을 만들어서 저장한다.\n",
    "# CB를 위한 Persontable을 category에 따라 pcikling\n",
    "# pearson_table을 만들기 위한 데이터 따로 저장\n",
    "df = pd.read_parquet(DATA_PATH + \"view_data.parquet.gzip\", columns = [\"category_code\"])\n",
    "df = df.fillna(\"missing\")\n",
    "df[\"category_code\"] = df[\"category_code\"].apply(lambda x : x.split(\".\"))\n",
    "df[\"category_code_0\"] = df[\"category_code\"].apply(lambda x : x[0])\n",
    "category_code_list = list(df[\"category_code_0\"].unique())\n",
    "category_code_list.remove(\"missing\")\n",
    "del df\n",
    "\n",
    "def df_by_category(target_category = \"all\"):\n",
    "    \"\"\"\n",
    "    매개변수\n",
    "    target_category : 1차 카테고리 코드를 지정해준다. all이면 모든 카테고리를 가져온다.\n",
    "    10개 이하의 product를 view한 user만 가져온다. \n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(DATA_PATH + \"view_data.parquet.gzip\", columns = [\"user_id\",\t\"event_type\",\t\"product_id\",\t\"category_id\",\t\"category_code\",\t\"brand\",\t\"price\"])\n",
    "    df = df.fillna(\"missing\")\n",
    "    df[\"event_type\"] = [1] * df.shape[0]\n",
    "    df.rename(columns = {'event_type':'event_type_view'},inplace=True)\n",
    "    df[\"category_code\"] = df[\"category_code\"].apply(lambda x : x.split(\".\"))\n",
    "    df[\"category_code_0\"] = df[\"category_code\"].apply(lambda x : x[0])\n",
    "    \n",
    "    # 카테고리에 적합한 df불러오기\n",
    "    if target_category != \"all\" :\n",
    "        df = df[df[\"category_code_0\"] == target_category]\n",
    "    df = df[df['brand'] != 'missing']\n",
    "    df = df[['user_id','event_type_view', 'product_id', 'category_id', 'category_code', 'brand', 'price', 'category_code_0']]\n",
    "    \n",
    "    # 기존 frame에 innerjoin\n",
    "    df_user_view = df.groupby(['user_id']).agg({'event_type_view' : 'sum'}).reset_index()\n",
    "    df_pick = pd.merge(left = df , right = df_user_view[['user_id']], how = \"inner\", on = \"user_id\")\n",
    "    del df_user_view , df\n",
    "    \n",
    "    # view한 product가 2개이상 10개 이하인 고객만 가져온다.\n",
    "    df_view_prod = df_pick.groupby(['user_id'])['product_id'].agg({'unique'})\n",
    "    df_view_prod['view_prod'] =df_view_prod['unique'].apply(lambda x: len(x))\n",
    "    df_view_prod = df_view_prod[(df_view_prod['view_prod'] >= 2) & (df_view_prod['view_prod'] <= 10)]\n",
    "    df_view_prod = df_view_prod.reset_index()\n",
    "    df_pick = pd.merge(left = df_pick , right = df_view_prod[['user_id']], how = \"inner\", on = \"user_id\")\n",
    "    del df_view_prod\n",
    "    \n",
    "    # prod_id별 mean_price 삽입\n",
    "    df_prod_id = df_pick.groupby(['product_id']).agg({'price' : 'mean'}).reset_index()\n",
    "    df_prod_id.rename(columns = {'price':'mean_price'},inplace=True)\n",
    "    df_pick = pd.merge(left = df_pick , right = df_prod_id, how = \"inner\", on = \"product_id\")\n",
    "    del df_prod_id\n",
    "    \n",
    "    # minMaxScale\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df_pick[['mean_price']])\n",
    "\n",
    "    mMscaled_data = scaler.transform(df_pick[['mean_price']])\n",
    "    mMscaled_data = pd.DataFrame(mMscaled_data, columns=['mMprice'])\n",
    "    df_pick = pd.concat([df_pick, mMscaled_data], axis= 1)\n",
    "    df_pick[\"category_code\"] = df_pick[\"category_code\"].apply(lambda x : str(x))\n",
    "    \n",
    "    del mMscaled_data, scaler\n",
    "    \n",
    "    return df_pick\n",
    "    \n",
    "def to_matrix(df):\n",
    "    \n",
    "    # Tfidf 사전 df생성\n",
    "    df = df[['product_id', 'category_code', 'brand','mean_price', 'mMprice']]\n",
    "    df = df.groupby(['product_id']).agg({'category_code' : 'unique', 'brand' : 'unique',\n",
    "                                        'mMprice' : 'unique', 'mean_price' : 'unique'}).reset_index()\n",
    "    df['category_code'] = df['category_code'].apply(lambda x: x[0])\n",
    "    df['category_code'] =df['category_code'].apply(lambda row: eval(''.join(row)))\n",
    "    df['category_code'] =df['category_code'].apply(lambda row: '.'.join(row))\n",
    "    df['brand'] = df['brand'].apply(lambda x: x[0])\n",
    "    df['mean_price'] = df['mean_price'].apply(lambda x: x[0])\n",
    "    df['mMprice'] = df['mMprice'].apply(lambda x: x[0])\n",
    "\n",
    "    cols = ['category_code', 'brand']\n",
    "    df['category_code+brand'] =df[cols].apply(lambda row: '.'.join(row.values.astype(str)), axis=1)\n",
    "    df = df[['product_id', 'category_code+brand', 'mean_price', 'mMprice']]\n",
    "    df['category_code+brand'] = df['category_code+brand'].str.replace('.',' ')\n",
    "    \n",
    "    vect = CountVectorizer()\n",
    "    docs = df['category_code+brand'].values\n",
    "    countvect = vect.fit_transform(docs)\n",
    "\n",
    "    # 벡터 데이터프레임화\n",
    "    countvect_df =pd.DataFrame(countvect.toarray(), columns = sorted(vect.vocabulary_))\n",
    "    countvect_df.index = df['product_id'].values\n",
    "    \n",
    "    df_tfidf_con = df[['mMprice']]\n",
    "    df_tfidf_con = df_tfidf_con.set_index(countvect_df.index)\n",
    "    countvect_df = pd.concat([countvect_df,df_tfidf_con], axis = 1)\n",
    "    del df_tfidf_con\n",
    "    \n",
    "    # pearson matrix 생성\n",
    "    pearson_sim = countvect_df.T.corr()\n",
    "    \n",
    "    del countvect_df, vect, docs\n",
    "\n",
    "    return pearson_sim        \n",
    "for category_code in category_code_list:\n",
    "    df = df_by_category(target_category = category_code)\n",
    "    df = to_matrix(df)\n",
    "    pickling(df, f\"{category_code}_pearson_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id_to_category_code.pkl로 pickling 완료\n"
     ]
    }
   ],
   "source": [
    "# product_id 입력받으면 해당 제품의 1차 카테고리를 반환하는 dict 생성\n",
    "df = pd.read_parquet(DATA_PATH + \"view_data.parquet.gzip\", columns = [\"product_id\", \"category_code\"])\n",
    "df = df.fillna(\"missing\")\n",
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=True)\n",
    "df[\"category_code\"] = df[\"category_code\"].apply(lambda x : x.split(\".\")[0])\n",
    "product_id_to_category_code = {product_id : category_code for product_id, category_code in list(zip(df[\"product_id\"], df[\"category_code\"]))}\n",
    "pickling(product_id_to_category_code, \"product_id_to_category_code\")\n",
    "del product_id_to_category_code, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개 이하, 2개 이상의 product 를 view한 user_id를 받으면 가장 view가 많은 상품을 반환하는 dict 저장 (단 category_code나 brand가 결측값인 로그는 제거한다)\n",
    "# user_to_most_viewed_product_id\n",
    "df = pd.read_parquet(DATA_PATH + \"view_data.parquet.gzip\", columns = [\"user_id\", \"product_id\", \"category_code\", \"brand\"])\n",
    "df = df.dropna().reset_index(drop =True)\n",
    "df = df[[\"user_id\", \"product_id\"]]\n",
    "df = df.groupby(\"user_id\").nunique()\n",
    "df = df[(df[\"product_id\"] >= 2) & (df[\"product_id\"] <= 10)]\n",
    "lower_user_list = df.index\n",
    "df = pd.read_parquet(DATA_PATH + \"view_data.parquet.gzip\", columns = [\"user_id\", \"product_id\", \"event_type\", \"category_code\", \"brand\"])\n",
    "df = df[df[\"user_id\"].isin(lower_user_list)]  \n",
    "df = df.dropna().reset_index(drop =True)\n",
    "df = df[[\"user_id\", \"product_id\", \"event_type\"]]\n",
    "df = df.groupby([\"user_id\", \"product_id\"]).count().reset_index()\n",
    "\n",
    "# 데이터를 csr_matrix로 만드는 과정입니다.\n",
    "user_unique = df['user_id'].unique()\n",
    "product_unique = df['product_id'].unique()\n",
    "cb_user_to_index = {user:index for index, user in enumerate(user_unique)}\n",
    "cb_index_to_user = {index:user for index, user in enumerate(user_unique)}\n",
    "cb_product_to_index = {product:index for index, product in enumerate(product_unique)}\n",
    "cb_index_to_product = {index:product for index, product in enumerate(product_unique)}\n",
    "\n",
    "# user와 product 의 index와의 변환을 위한 dict pickling\n",
    "df['user_id'] = df['user_id'].map(cb_user_to_index.get)\n",
    "df['product_id'] = df['product_id'].map(cb_product_to_index.get)\n",
    "num_user = df['user_id'].nunique()\n",
    "num_product = df['product_id'].nunique()\n",
    "lower_user_item_matrix = csr_matrix((df.event_type, (df.user_id, df.product_id)), shape= (num_user, num_product))\n",
    "\n",
    "del df, num_product, num_user, user_unique, product_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능 평가를 위해 한 유저당 임의의 상품 하나의 view기록을 0으로 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d36430e57f458292c42ce28fa883a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1177302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 각 유저마다 랜덤하게 하나씩 0으로 가리는 작업입니다.\n",
    "samples = []\n",
    "\n",
    "for user_idx in tqdm(range(lower_user_item_matrix.shape[0])) :\n",
    "    samples.append((user_idx, random.sample(lower_user_item_matrix[user_idx].nonzero()[1].tolist(), 1)[0]))\n",
    "    \n",
    "training_set = lower_user_item_matrix.copy()\n",
    "test_set = lower_user_item_matrix.copy()\n",
    "\n",
    "user_inds = [index[0] for index in samples]\n",
    "item_inds = [index[1] for index in samples]\n",
    "\n",
    "training_set[user_inds, item_inds] = 0\n",
    "training_set.eliminate_zeros()\n",
    "\n",
    "del lower_user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저별로 가장 view 수가 큰 product_id를 가지는 list\n",
    "input_data = list(np.array(np.argmax(training_set, axis=1)).reshape(-1))\n",
    "input_data = list(map(cb_index_to_product.get, input_data))\n",
    "\n",
    "# 가려진 product_id를 가지는 list\n",
    "label = list(map(cb_index_to_product.get, item_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9422599202c7467dbf41fc59a8f52893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 표본으로 정확도 검증 하기\n",
    "df = pd.read_parquet(DATA_PATH + \"view_data.parquet.gzip\", columns = [\"product_id\", \"event_type\"])\n",
    "df = df.groupby(\"product_id\").count()\n",
    "popular_product_id_list = list(df.sort_values(\"event_type\", ascending=False).index[:10])\n",
    "del df\n",
    "product_id_to_category_code = test_pkl(\"product_id_to_category_code\")\n",
    "answer_store_by_model = []\n",
    "answer_store_by_pop = []\n",
    "sample_size = 10000\n",
    "for user_index in tqdm(np.random.randint(training_set.shape[1], size=sample_size)):\n",
    "    input_product_id = input_data[user_index]\n",
    "    input_category_code = product_id_to_category_code[input_product_id]\n",
    "    pearson_table = test_pkl(f\"{input_category_code}_pearson_table\")\n",
    "    viewed_product_index_list = list(np.where(training_set[user_index].toarray()[0] != 0)[0])\n",
    "    viewed_product_id_list = list(map(cb_index_to_product.get, viewed_product_index_list ))\n",
    "    pearson_table = pearson_table[~pearson_table.index.isin(viewed_product_id_list)]\n",
    "    answer_by_model = label[user_index] in list(pearson_table[input_product_id].sort_values(ascending=False).index[:10])\n",
    "    answer_store_by_model.append(answer_by_model)\n",
    "    answer_by_pop = label[user_index] in popular_product_id_list\n",
    "    answer_store_by_pop.append(answer_by_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검정통계량 값 = 3.5819902492699147\n",
      "유의확률 = 0.00017049326141016508\n"
     ]
    }
   ],
   "source": [
    "# 귀무가설 : cb 모델을 사용하는것의 정확도와 가장조회수가 높은 것을 추천하는것의 정확도보다 작거나 같다.\n",
    "# 대립가설 : cb 모델을 사용하는것의 정확도가 가장조회수가 높은 것을 추천하는것의 정확도보다 크다.\n",
    "\n",
    "model = sum(answer_store_by_model)/len(answer_store_by_model)\n",
    "pop = sum(answer_store_by_pop)/len(answer_store_by_model)\n",
    "pool = (sample_size * (model + pop)) / (sample_size * 2)\n",
    "Z = (model - pop) / np.sqrt(pool * (1 - pool) * (1/sample_size + 1/sample_size))\n",
    "print(f\"검정통계량 값 = {Z}\")\n",
    "print(f\"유의확률 = {1 - norm.cdf(Z)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유의수준 0.01 하에서 귀무가설을 기각한다.  \n",
    "cb 모델을 사용하는것의 정확도가 가장조회수가 높은 것을 추천하는것의 정확도보다 크다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
